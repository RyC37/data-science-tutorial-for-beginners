本章我们会介绍一个简单的分类算法，K-Nearest-Neighbors (简称KNN)。我们会介绍KNN的原理，以及训练模型的方法。然后，我们会分析（现实中的例子），来说明KNN的工作原理，并且验证它的效果。



## 物以类聚KNN

KNN是一个简单的算法，却有不错的分类能力，经常作为基准与其他复杂的算法（比如神经网络）比较。

KNN的概念是“物以类聚”：当需要给新的数据进行分类时，KNN会根据特征找到相似的observation，然后根据这些相似observation的分类决定新数据的分类。由此可见，KNN算法对数据的假设是：特征相似的observation，具有相似的分类。

以上就是KNN的主要思想。由于KNN原理简单，所以模型的结果也很好让其他领域的人理解。这也是KNN能够在金融和医疗领域中应用的主要原因。



## KNN的原理

K Nearest Neighbour意思是“K个最相似的observation”。由于KNN的分类依照相似observation的分类决定当前observation的类别，所以选择多少个observation进行参考，会影响到结果。所以如何决定K的值，是KNN模型的关键。

假设我们有一个中学生的身高体重数据库，可视化结果如下：

![image-20190716173058261](/Users/sicongzhao/Library/Application Support/typora-user-images/image-20190716173058261.png)

蓝色的点表示初中生，红色的点表示高中生。绿色的圆环代表我们需要预测的新数据。

首先，我们令K=3。从下图可以看到，距离我们要预测的数据最近的3个observation，有2个是初中生，1个是高中生。所以，当K=3时，此时KNN会给将新数据分类为初中生。

![image-20190716173927462](/Users/sicongzhao/Library/Application Support/typora-user-images/image-20190716173927462.png)

但如果我们改变K的值，这个结果也有可能改变。譬如，如果我们令K等于6，就会发现距离预测数据最近的6个observation中，有4个是高中生，2个是初中生。所以，当K=6时，KNN会将新数据分类为高中生。

![image-20190716174219505](/Users/sicongzhao/Library/Application Support/typora-user-images/image-20190716174219505.png)

以上，希望你理解了KNN的工作原理。接下来，我们介绍如何选择K。



## 决策边界（Decision Boundary）

首先，引入一个概念叫做Decision Boundary（决策边界）。

Decision Boundary的作用是显示坐标轴中的分类情况。如下图中，假设我们要预测新的数据，如果某位同学的身高和体重在坐标轴中位于蓝色的部分，那么KNN算法会预测该同学属于“初中生”；如果某位同学的身高和体重在坐标轴中位于红色的部分，那么KNN算法会预测该同学属于“高中生”。

基于中学生的数据库，我们画出了当K=3和K=6的时候，两个决策边界。

![foo3uniform](/Users/sicongzhao/Library/Mobile Documents/com~apple~CloudDocs/Project/DS Tutorial/jupyter-notebook/notebooks/foo3uniform.png)

![foo6uniform](/Users/sicongzhao/Library/Mobile Documents/com~apple~CloudDocs/Project/DS Tutorial/jupyter-notebook/notebooks/foo6uniform.png)

可以看到，当K为3的时候，决策边界（decision boundary）相对更蜿蜒复杂；而当K为6的时候，决策边界（decision boundary）就会平缓很多。

所以，通过绘制决策边界，我们更能够理解K值对于算法表现的影响。接下来，我们就来讨论如何选择K值。



## 过拟合 Over-fitting

在训练模型的过程中，我们如何评估模型的表现呢？

在Simple Linear Regression中，我们使用了训练数据进行结果的评估。这样的评估是不客观的，由于这些模型“学习”过这些数据，所以其表现有可能会高于实际水平。

所以，使用训练数据进行评估，不能客观的衡量模型的表现。为了解决这个问题，在训练模型之前，我们通常会将一份数据集随机拆分成两个部分：**训练集(Training Set)**和**测试集(Test Set)**。

**训练集(Training Set)**通常占总数据量的70%，只用于模型的训练。

**测试集(Test Set)**通常占总数据量的30%，用于测试训练完成的模型表现。

![image-20190716232308374](/Users/sicongzhao/Library/Application Support/typora-user-images/image-20190716232308374.png)

理想情况下，模型在训练集中的表现应该和在测试集中类似。但是如果模型在训练集中的表现远远高于在测试集中的表现，这说明我们的模型过度的“吸取”了训练数据中的信息，所以不适用于测试数据。这就像是在不理解知识的情况下，通过“死记硬背答案”来备考，只有考试遇到同样的问题才能得分。如果问题稍微改变一些，就没有办法给出正确答案。

这种情况下，我们称作模型**过拟合（Over-fitting）**



## K的选择

通常情况下，我们使用穷举法，测试不同的K值，然后找出测试集中表现最好的K值。

如果作出精度～K值的曲线，图形如下：

![image-20190716234205253](/Users/sicongzhao/Library/Application Support/typora-user-images/image-20190716234205253.png)

当K值越小，模型就越灵活，就越容易出现过拟合的情况。实际上，当K等于1的时候，如果用训练数据进行测试会发现精度是100%。随着K的增加，模型在测试集中的表现逐渐提升，分类错误率（Error Rate）会达到最小值。但过了一定的阶段，K值过大，就会考虑太多的影响。极端情况是当K值等于所有observation的数量时，无论输入数据是什么，结果都是同一种。这种情况下，我们叫作**欠拟合（Under-fitting）**。

所以，在实际选择K的过程中，我们会逐一尝试可能的K值，画出模型在测试集中错误率随着K值变化的曲线。然后选择错误率最小的K值。



## KNN用于回归（Regression）

除了用于分类，KNN还可以用于预测。思路是取N个最近的observation，然后计算output的平均值。

假设我们现在要根据受教育程度(Years of Education)和工作经验(Years of Work Experience)来预测一个人的工资（绿色圆环表示）。

当K等于3时，找到离他最近的3个observation，然后计算平均年薪（红色标签表示）。所以在这个问题中，KNN算法会预测这个人的年薪为：$(40 + 25 + 30) / 3 \approx 31.7 \text{K}$

![image-20190716235759623](/Users/sicongzhao/Library/Application Support/typora-user-images/image-20190716235759623.png)



## 总结

本章我们学习了：

* KNN的原理
* KNN用于分类
* KNN用于回归
* 决策边界的概念
* 如何选择K值

在实际使用中，KNN的优势在于：

* 简单易懂
* 不需要训练模型，也不需要对数据进行任何假设（比如假设数据中有线性关系）
* 可以进行分类和回归

其缺点是：

* 当我们有很多predictor的时候，这个算法会非常慢。因为当有多维度的feature的时候，计算observation之间的距离会需要很长时间。

接下来，我们进入动手环节，看看KNN的实战能力怎么样。















